services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: backend
    ports:
      - "8000:8000"
    volumes:
      - ./:/app
      - /app/.venv/

    tty: true
    command: >
      /bin/sh -c "
      if [ ! -f .env ]; then cp .env.example .env; fi &&
      make install-uv &&
      make install-backend-cuda &&
      make run-backend
      "
    networks:
      - localnetwork
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 15s
      timeout: 3s
      retries: 20
      start_period: 30s
    env_file:
      - .env
    environment:
      BACKEND_HOST: ${BACKEND_HOST:-0.0.0.0}
      BACKEND_PORT: ${BACKEND_PORT:-8000}
      OLLAMA_MODEL_NAME: ${OLLAMA_MODEL_NAME:-qwen3:0.6b}
      OLLAMA_EMBEDDING_MODEL_NAME: ${OLLAMA_EMBEDDING_MODEL_NAME:-all-minilm:l6-v2}
      INFERENCE_DEPLOYMENT_NAME: ${INFERENCE_DEPLOYMENT_NAME:-ollama_chat/qwen2.5:0.5b}
      INFERENCE_BASE_URL: ${INFERENCE_BASE_URL:-http://ollama:11434}
      INFERENCE_API_KEY: ${INFERENCE_API_KEY:-t}
      EMBEDDINGS_DEPLOYMENT_NAME: ${EMBEDDINGS_DEPLOYMENT_NAME:-ollama/all-minilm:l6-v2}
      EMBEDDINGS_BASE_URL: ${EMBEDDINGS_BASE_URL:-http://ollama:11434}
      EMBEDDINGS_API_KEY: ${EMBEDDINGS_API_KEY:-t}
      UV_PROJECT_ENVIRONMENT: /venv-backend

  frontend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: frontend
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "8080:8080"
    volumes:
      - /app/.venv/
      - ./:/app
    tty: true
    command: >
      /bin/sh -c "
      make install-uv &&
      make install-frontend &&
      make run-frontend
      "
    networks:
      - localnetwork
    env_file:
      - .env
    environment:
      BACKEND_URL: ${BACKEND_URL:-http://backend:8000}
      UV_PROJECT_ENVIRONMENT: /venv-frontend


  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    entrypoint: [""]
    ports:
      - 11434:11434
    volumes:
      - ../genai_template_data/ollama/:/root/.ollama
      - ./:/app/
    container_name: ollama
    pull_policy: always
    tty: true

    # use tail -f /dev/null in command to keep the container running
    command: >
      /bin/sh -c "
      make download-ollama-models &&
      tail -f /dev/null"
    env_file:
      - .env

networks:
  localnetwork:
    driver: bridge
    name: localnetwork
